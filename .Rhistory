# and convert to lower case:
sentence <- tolower(sentence)
# split into words. str_split is in the stringr package
word.list <- str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words <- unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches <- match(words, pos.words)
neg.matches <- match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
# TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words)
scores <- unlist(scores)
sentences <- unlist(sentences)
scores.df <- data.frame(score=scores, text=sentences)
return(scores.df)
}
analysisSentiment <- sentimentScore(sentences = tweetsSubSet$text, pos.words = positiveWords,
neg.words = negativeWords)
analysisSentiment %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "lightblue")+
ylab("Frequency") +
xlab("sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
ggeasy::easy_center_title()
neutral <- length(which(analysisSentiment$score == 0))
positive <- length(which(analysisSentiment$score > 0))
negative <- length(which(analysisSentiment$score < 0))
Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
geom_bar(stat = "identity", aes(fill = Sentiment))+
ggtitle("Barplot of Sentiment for inflation")
text_corpus <- Corpus(VectorSource(tweetsSubSet$text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, function(x)removeWords(x,stopwords("english")))
text_corpus <- tm_map(text_corpus, removeWords, c("inflation"))
tdm <- TermDocumentMatrix(text_corpus)
tdm <- as.matrix(tdm)
tdm <- sort(rowSums(tdm), decreasing = TRUE)
tdm <- data.frame(word = names(tdm), freq = tdm)
set.seed(123)
wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
tweetsSubSet
cloudW <- wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
cloudW
ggplot(tdm[1:20,], aes(x=reorder(word, freq), y=freq)) +
geom_bar(stat="identity") +
xlab("Terms") +
ylab("Count") +
coord_flip() +
theme(axis.text=element_text(size=7)) +
ggtitle('Most common word frequency plot') +
ggeasy::easy_center_title()
text_corpus <- tm_map(text_corpus, removeWords, c("inflation", "will", "can"))
tdm <- TermDocumentMatrix(text_corpus)
tdm <- as.matrix(tdm)
tdm <- sort(rowSums(tdm), decreasing = TRUE)
tdm <- data.frame(word = names(tdm), freq = tdm)
set.seed(123)
wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
ggplot(tdm[1:20,], aes(x=reorder(word, freq), y=freq)) +
geom_bar(stat="identity") +
xlab("Terms") +
ylab("Count") +
coord_flip() +
theme(axis.text=element_text(size=7)) +
ggtitle('Most common word frequency plot') +
ggeasy::easy_center_title()
#bigram
bi.gram.words <- tweetsSubSet %>%
unnest_tokens(
input = text,
output = bigram,
token = 'ngrams',
n = 2
) %>%
filter(! is.na(bigram))
bi.gram.words %>%
select(bigram) %>%
head(10)
extra.stop.words <- c('https')
stopwords.df <- tibble(
word = c(stopwords(kind = 'es'),
stopwords(kind = 'en'),
extra.stop.words)
)
bi.gram.words %<>%
separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>%
filter(! word1 %in% stopwords.df$word) %>%
filter(! word2 %in% stopwords.df$word) %>%
filter(! is.na(word1)) %>%
filter(! is.na(word2))
bi.gram.count <- bi.gram.words %>%
dplyr::count(word1, word2, sort = TRUE) %>%
dplyr::rename(weight = n)
bi.gram.count %>% head()
threshold <- 50
# For visualization purposes we scale by a global factor.
ScaleWeight <- function(x, lambda) {
x / lambda
}
network <-  bi.gram.count %>%
filter(weight > threshold) %>%
mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>%
graph_from_data_frame(directed = FALSE)
plot(
network,
vertex.size = 1,
vertex.label.color = 'black',
vertex.label.cex = 0.7,
vertex.label.dist = 1,
edge.color = 'gray',
main = 'Bigram Count Network',
sub = glue('Weight Threshold: {threshold}'),
alpha = 50
)
V(network)$degree <- strength(graph = network)
# Compute the weight shares.
E(network)$width <- E(network)$weight/max(E(network)$weight)
plot(
network,
vertex.color = 'lightblue',
# Scale node size by degree.
vertex.size = 2*V(network)$degree,
vertex.label.color = 'black',
vertex.label.cex = 0.6,
vertex.label.dist = 1.6,
edge.color = 'gray',
# Set edge width proportional to the weight relative value.
edge.width = 3*E(network)$width ,
main = 'Bigram Count Network',
sub = glue('Weight Threshold: {threshold}'),
alpha = 50
)
threshold <- 50
network <-  bi.gram.count %>%
filter(weight > threshold) %>%
graph_from_data_frame(directed = FALSE)
# Store the degree.
V(network)$degree <- strength(graph = network)
# Compute the weight shares.
E(network)$width <- E(network)$weight/max(E(network)$weight)
# Create networkD3 object.
network.D3 <- igraph_to_networkD3(g = network)
# Define node size.
network.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)
# Define color group
network.D3$nodes %<>% mutate(Group = 1)
# Define node size.
network.D3$nodes %>% mutate(Degree = (1E-2)*V(network)$degree)
# Define color group
network.D3$nodes %>% mutate(Group = 1)
# Define edges width.
network.D3$links$Width <- 10*E(network)$width
forceNetwork(
Links = network.D3$links,
Nodes = network.D3$nodes,
Source = 'source',
Target = 'target',
NodeID = 'name',
Group = 'Group',
opacity = 0.9,
Value = 'Width',
Nodesize = 'Degree',
# We input a JavaScript function.
linkWidth = JS("function(d) { return Math.sqrt(d.value); }"),
fontSize = 12,
zoom = TRUE,
opacityNoHover = 1
)
network.D3
forceNetwork(
Links = network.D3$links,
Nodes = network.D3$nodes,
Source = 'source',
Target = 'target',
NodeID = 'name',
Group = 'Group',
opacity = 0.9,
Value = 'Width',
Nodesize = 'Degree',
# We input a JavaScript function.
linkWidth = JS("function(d) { return Math.sqrt(d.value); }"),
fontSize = 12,
zoom = TRUE,
opacityNoHover = 1
)
forceNetwork(
Links = network.D3$links,
Nodes = network.D3$nodes,
Source = 'source',
Target = 'target',
NodeID = 'name',
opacity = 0.9,
Value = 'Width',
Nodesize = 'Degree',
# We input a JavaScript function.
linkWidth = JS("function(d) { return Math.sqrt(d.value); }"),
fontSize = 12,
zoom = TRUE,
opacityNoHover = 1
)
forceNetwork(
Links = network.D3$links,
Nodes = network.D3$nodes,
Source = 'source',
Target = 'target',
NodeID = 'name',
opacity = 0.9,
Value = 'Width',
# We input a JavaScript function.
linkWidth = JS("function(d) { return Math.sqrt(d.value); }"),
fontSize = 12,
zoom = TRUE,
opacityNoHover = 1
)
librarian::shelf(tidyverse, reshape2, readxl, data.table, nleqslv, BB, Metrics, ggthemes, pracma,
twitteR, ROAuth, hms, lubridate, tidytext, tm, wordcloud, igraph, glue, networkD3,
rtweet, stringr, ggeasy, plotly, janeaustenr, widyr, textdata)
tweets
text_corpus
install.packages("tm")
librarian::shelf(tm, SnowballC, wordcloud, readtext)
wordbase <- readtext("DineshPoddaturi-ProposalPacket.pdf")
librarian::shelf(tm, SnowballC, wordcloud, readtext, pdftools)
install.packages("pdftools")
librarian::shelf(tm, SnowballC, wordcloud, readtext, pdftools)
library(pdftools)
readtext("DineshPoddaturi-ProposalPacket.pdf")
pdf_text("DineshPoddaturi-ProposalPacket.pdf")
remove.packages("pdftools")
install.packages("pdftools")
library(pdftools)
pdftools::pdf_text("DineshPoddaturi-ProposalPacket.pdf")
install.packages("pdftools")
install.packages("wordcloud")
install.packages("wordcloud")
.libPaths()
.libPaths()
librarian::shelf(tidyverse, reshape2, readxl, data.table, nleqslv, BB, Metrics, ggthemes, pracma,
twitteR, ROAuth, hms, lubridate, tidytext, tm, wordcloud, igraph, glue, networkD3,
rtweet, stringr, ggeasy, plotly, janeaustenr, widyr, textdata)
library(pdftools)
install.packages("pdftools")
require(pdftools)
brew install poppler
install.packages('pdftools', type='source')
install.packages("pdftools", type = "source")
library(pdftools)
pdf_text("DineshPoddaturi-ProposalPacket.pdf")
librarian::shelf(tidyverse, reshape2, readxl, data.table, nleqslv, BB, Metrics, ggthemes, pracma,
twitteR, ROAuth, hms, lubridate, tidytext, tm, wordcloud, igraph, glue, networkD3,
rtweet, stringr, ggeasy, plotly, janeaustenr, widyr, textdata)
librarian::shelf(tm, SnowballC, wordcloud, readtext, pdftools)
wordbase <- readtext("DineshPoddaturi-ProposalPacket.pdf")
print(wordbase)
wordbase[1, 2]
corp <- Corpus(VectorSource(wordbase))
corp <- tm_map(corp, PlainTextDocument)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, tolower)
removeNumbers
corp <- tm_map(corp, removeWords, stopwords(kind = "en"))
corp
wordcloud(corp, max.words = 100, random.order = FALSE)
corp
corp$`1`
removeWords
txt <- pdf_text("path/file.pdf")
txt <- pdf_text("DineshPoddaturi-ProposalPacket.pdf")
txt
docs <- Corpus(VectorSource(txt))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
d
d %>% filter(freq>=50)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordsDiss <- c(replicate(2, "my_string"), replicate(2, "my_string1"))
wordsDiss
table(wordsDiss)
count(wordsDiss)
str_count(wordsDiss, "a")
wordsDiss
wordsDiss <- c(replicate(2, "my_string"), replicate(2, "my_string1")) %>% as.data.frame()
wordsDiss
names(wordsDiss) <- "words"
str_count(wordsDiss, "a")
str_count(wordsDiss$words, "a")
wordsDiss <- c(replicate(2, "my_string"), replicate(2, "my_string1")) %>% as.data.frame()
4/5
wordsDiss <- c(replicate(2, "my_string"), replicate(2, "my_string1")) %>% as.data.frame()
librarian::shelf(tidyverse, reshape2, readxl, data.table, nleqslv, BB, Metrics, ggthemes, pracma,
twitteR, ROAuth, hms, lubridate, tidytext, tm, wordcloud, igraph, glue, networkD3,
rtweet, stringr, ggeasy, plotly, janeaustenr, widyr, textdata)
librarian::shelf(tm, SnowballC, wordcloud, readtext, pdftools)
wordsDiss <- c(replicate(2, "my_string"), replicate(2, "my_string1")) %>% as.data.frame()
names(wordsDiss) <- "words"
str_count(wordsDiss$words, "a")
wordsDiss
wordsDiss %>% group_by(words) %>% mutate(occ = count(words))
?count
wordsDiss %>% group_by(words) %>% tally()
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
txt <- pdf_text("DineshPoddaturi-ProposalPacket.pdf")
docs <- Corpus(VectorSource(txt))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
d %>% filter(freq>=50)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordsDiss <- c(replicate(200, "economic"), replicate(180, "expectations"), replicate(160, "dynamic"),
replicate(170, "model"), replicate(190, "cattle"), replicate(150, "price"),
replicate(140, "production"),replicate(130, "farmers"), replicate(150, "demand"),
replicate(160, "impacts"), replicate(160, "supply"),replicate(160, "equilibrium"),
replicate(160, "rational"), replicate(160, "years"),replicate(160, "industry"),
replicate(160, "cow"), replicate(160, "beef"),replicate(160, "solve"),
replicate(160, "algorithm"), replicate(160, "distribution"), replicate(160, "age"),
replicate(160, "parameters"), replicate(160, "solution"),replicate(160, "behavior"),
replicate(160, "meat"), replicate(160, "interpolation"),replicate(160, "literature"),
replicate(160, "data"), replicate(160, "inventories"),replicate(160, "non-linear"),
replicate(160, "decisions"), replicate(160, "numerical"),replicate(160, "quantities"),
replicate(160, "observed"), replicate(160, "projected"),replicate(160, "confidence"),
replicate(160, "disease"), replicate(160, "variables"),replicate(160, "fitted"),
replicate(160, "developed"), replicate(160, "markets"),replicate(160, "constructed"),
replicate(160, "consumers"), replicate(160, "year"),replicate(160, "animal"),
replicate(160, "available"), replicate(160, "usda"),replicate(160, "dynamics")) %>% as.data.frame()
names(wordsDiss) <- "words"
wordsDiss %>% group_by(words) %>% tally()
wordsDiss %>% group_by(words) %>% tally() %>% as.data.frame()
wordsDiss <- wordsDiss %>% group_by(words) %>% tally() %>% as.data.frame()
wordcloud(words = wordsDiss$words, freq = wordsDiss$n, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
d
wordsDiss <- c(replicate(315, "economic"), replicate(300, "expectations"),
replicate(290, "dynamic"), replicate(280, "structural"),
replicate(270, "model"), replicate(260, "price"),
replicate(250, "consumer"), replicate(240, "dynamic"),
replicate(230, "production"), replicate(220, "supply"),
replicate(220, "demand"), replicate(200, "data"),
replicate(190, "impacts"), replicate(190, "expected"),
replicate(200, "equilibrium"), replicate(150, "numerical"),
replicate(180, "rational"), replicate(100, "industry"),
replicate(140, "solve"), replicate(130, "algorithm"),
replicate(120, "distribution"), replicate(110, "equations"),
replicate(90, "parameters"), replicate(92, "solution"),
replicate(105, "behavior"), replicate(95, "estimation"),
replicate(50, "interpolation"),replicate(94, "literature"),
replicate(30, "inventories"),replicate(100, "non-linear"),
replicate(160, "decisions"), replicate(125, "numerical"),
replicate(90, "quantities"), replicate(91, "future"),
replicate(89, "observed"), replicate(89, "projected"),
replicate(85, "confidence"), replicate(83, "process"),
replicate(80, "variables"),replicate(78, "fitted"),
replicate(75, "developed"), replicate(265, "markets"),
replicate(74, "constructed"), replicate(70, "value"),
replicate(65, "year"), replicate(60, "available"),
replicate(62, "annual"), replicate(160, "naïve"),
replicate(60, "changes"), replicate(78, "assume"),
replicate(60, "framework"), replicate(165, "exogenous"),
replicate(160, "endogenous"), replicate(100, "competitive"),
replicate(58, "response"), replicate(83, "system")) %>% as.data.frame()
names(wordsDiss) <- "words"
wordsDiss <- wordsDiss %>% group_by(words) %>% tally() %>% as.data.frame()
wordsDiss
wordcloud(words = wordsDiss$words, freq = wordsDiss$n, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordsDiss <- c(replicate(315, "economic"), replicate(300, "expectations"),
replicate(290, "dynamic"), replicate(280, "structural"),
replicate(270, "model"), replicate(260, "price"),
replicate(250, "consumer"), replicate(240, "dynamics"),
replicate(230, "production"), replicate(220, "supply"),
replicate(220, "demand"), replicate(200, "data"),
replicate(190, "impacts"), replicate(190, "expected"),
replicate(200, "equilibrium"), replicate(150, "numerical"),
replicate(180, "rational"), replicate(100, "industry"),
replicate(140, "solve"), replicate(130, "algorithm"),
replicate(120, "distribution"), replicate(110, "equations"),
replicate(90, "parameters"), replicate(92, "solution"),
replicate(105, "behavior"), replicate(95, "estimation"),
replicate(50, "interpolation"),replicate(94, "literature"),
replicate(30, "inventories"),replicate(100, "non-linear"),
replicate(160, "decisions"), replicate(125, "numerical"),
replicate(90, "quantities"), replicate(91, "future"),
replicate(89, "observed"), replicate(89, "projected"),
replicate(85, "confidence"), replicate(83, "process"),
replicate(80, "variables"),replicate(78, "fitted"),
replicate(75, "developed"), replicate(265, "markets"),
replicate(74, "constructed"), replicate(70, "value"),
replicate(65, "year"), replicate(60, "available"),
replicate(62, "annual"), replicate(160, "naïve"),
replicate(60, "changes"), replicate(78, "assume"),
replicate(60, "framework"), replicate(165, "exogenous"),
replicate(160, "endogenous"), replicate(100, "competitive"),
replicate(58, "response"), replicate(83, "system")) %>% as.data.frame()
names(wordsDiss) <- "words"
wordsDiss <- wordsDiss %>% group_by(words) %>% tally() %>% as.data.frame()
wordcloud(words = wordsDiss$words, freq = wordsDiss$n, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
plt %>% ggplotly()
analysisSentiment %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "lightblue")+
ylab("Frequency") +
xlab("sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
ggeasy::easy_center_title()
neutral <- length(which(analysisSentiment$score == 0))
positive <- length(which(analysisSentiment$score > 0))
negative <- length(which(analysisSentiment$score < 0))
Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
geom_bar(stat = "identity", aes(fill = Sentiment))+
ggtitle("Barplot of Sentiment for inflation")
text_corpus <- Corpus(VectorSource(tweetsSubSet$text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, function(x)removeWords(x,stopwords("english")))
text_corpus <- tm_map(text_corpus, removeWords, c("inflation", "will", "can"))
tdm <- TermDocumentMatrix(text_corpus)
tdm <- as.matrix(tdm)
tdm <- sort(rowSums(tdm), decreasing = TRUE)
tdm <- data.frame(word = names(tdm), freq = tdm)
set.seed(123)
wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
get_sentiments("afinn")
#Note: Replace below with your credentials following above reference
api_key <- "oJ6CmPerh7jHxP5ZHazRuez1U"
library(openssl)
library(httpuv)
library(rtweet)
#Note: Replace below with your credentials following above reference
api_key <- "oJ6CmPerh7jHxP5ZHazRuez1U"
api_secret_key <- "zVKRrtlXKdWBPsFKaBs4h6z9cS0nQrZwoRI6dmhfDcdV3qDS20"
access_token <- "3914872158-vYvnOj6VE5ZpXcexVAu3YcMPh54uVdtfu4LLGVk"
access_token_secret <- "jhC9VDAZNbFj4mpPoWbRU3eq1KCrghMqIeiMpkCylcG8i"
#Note: This will ask us permission for direct authentication, type '1' for yes:
# setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
library(rtweet)
## authenticate via web browser
token <- create_token(
app = "RangoUnchained",
consumer_key = api_key,
consumer_secret = api_secret_key,
access_token = access_token,
access_secret = access_token_secret)
