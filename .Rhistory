access_token <- "3914872158-vYvnOj6VE5ZpXcexVAu3YcMPh54uVdtfu4LLGVk"
access_token_secret <- "jhC9VDAZNbFj4mpPoWbRU3eq1KCrghMqIeiMpkCylcG8i"
#Note: This will ask us permission for direct authentication, type '1' for yes:
# setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
library(rtweet)
## authenticate via web browser
token <- create_token(
app = "RangoUnchained",
consumer_key = api_key,
consumer_secret = api_secret_key,
access_token = access_token,
access_secret = access_token_secret)
librarian::shelf(tidyverse, reshape2, readxl, data.table, nleqslv, BB, Metrics, ggthemes, pracma,
twitteR, ROAuth, hms, lubridate, tidytext, tm, wordcloud, igraph, glue, networkD3,
rtweet, stringr, ggeasy, plotly, janeaustenr, widyr, textdata)
if (!requireNamespace("httpuv", quietly = TRUE)) {
install.packages("httpuv")
}
library(openssl)
library(httpuv)
library(rtweet)
#Note: Replace below with your credentials following above reference
api_key <- "oJ6CmPerh7jHxP5ZHazRuez1U"
api_secret_key <- "zVKRrtlXKdWBPsFKaBs4h6z9cS0nQrZwoRI6dmhfDcdV3qDS20"
access_token <- "3914872158-vYvnOj6VE5ZpXcexVAu3YcMPh54uVdtfu4LLGVk"
access_token_secret <- "jhC9VDAZNbFj4mpPoWbRU3eq1KCrghMqIeiMpkCylcG8i"
#Note: This will ask us permission for direct authentication, type '1' for yes:
# setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
library(rtweet)
## authenticate via web browser
token <- create_token(
app = "RangoUnchained",
consumer_key = api_key,
consumer_secret = api_secret_key,
access_token = access_token,
access_secret = access_token_secret)
## pre-processing text: This function is to clean the text of the tweet of
## any symbols, and other unnecessary items that are non-readable
cleanText <- function(x){
# convert to lower case
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
# some other cleaning text
x = gsub('https://','',x)
x = gsub('http://','',x)
x = gsub('[^[:graph:]]', ' ',x)
x = gsub('[[:punct:]]', '', x)
x = gsub('[[:cntrl:]]', '', x)
x = gsub('\\d+', '', x)
x = str_replace_all(x,"[^[:graph:]]", " ")
return(x)
}
search_term <- "#recession"
by <- 'hour'
tweets <- search_tweets(q = search_term ,
n = 1000000, retryonratelimit = TRUE, lang="en", include_rts = FALSE)
nrow(tweets)
search_term <- "#recession"
by <- 'hour'
tweets <- search_tweets(q = search_term ,
n = 100000, retryonratelimit = TRUE, lang="en", include_rts = FALSE)
tweets <- tweets %>% as.data.frame()
nrow(tweets)
tweets <- searchTwitter(search_term, n=4000, lang="en")
#Note: Replace below with your credentials following above reference
api_key <- "oJ6CmPerh7jHxP5ZHazRuez1U"
api_secret_key <- "zVKRrtlXKdWBPsFKaBs4h6z9cS0nQrZwoRI6dmhfDcdV3qDS20"
access_token <- "3914872158-vYvnOj6VE5ZpXcexVAu3YcMPh54uVdtfu4LLGVk"
access_token_secret <- "jhC9VDAZNbFj4mpPoWbRU3eq1KCrghMqIeiMpkCylcG8i"
setup_twitter_oauth(api_key, api_secret_key, access_token, access_token_secret)
#Note: This will ask us permission for direct authentication, type '1' for yes:
# setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
library(rtweet)
## authenticate via web browser
token <- create_token(
app = "RangoUnchained",
consumer_key = api_key,
consumer_secret = api_secret_key,
access_token = access_token,
access_secret = access_token_secret)
## pre-processing text: This function is to clean the text of the tweet of
## any symbols, and other unnecessary items that are non-readable
cleanText <- function(x){
# convert to lower case
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
# some other cleaning text
x = gsub('https://','',x)
x = gsub('http://','',x)
x = gsub('[^[:graph:]]', ' ',x)
x = gsub('[[:punct:]]', '', x)
x = gsub('[[:cntrl:]]', '', x)
x = gsub('\\d+', '', x)
x = str_replace_all(x,"[^[:graph:]]", " ")
return(x)
}
search_term <- "#recession"
by <- 'hour'
tweets <- searchTwitter(search_term, n=4000, lang="en")
tweets <- searchTwitter(searchString = search_term, n=100000, lang="en")
tweets
names(tweets)
tweets <- searchTwitter(searchString = search_term, n=100000, lang="en", retryonratelimit = TRUE)
?searchTwitter
api_key <- "oJ6CmPerh7jHxP5ZHazRuez1U"
api_secret_key <- "zVKRrtlXKdWBPsFKaBs4h6z9cS0nQrZwoRI6dmhfDcdV3qDS20"
access_token <- "3914872158-vYvnOj6VE5ZpXcexVAu3YcMPh54uVdtfu4LLGVk"
access_token_secret <- "jhC9VDAZNbFj4mpPoWbRU3eq1KCrghMqIeiMpkCylcG8i"
## authenticate via web browser
token <- create_token(
app = "RangoUnchained",
consumer_key = api_key,
consumer_secret = api_secret_key,
access_token = access_token,
access_secret = access_token_secret)
search_term <- "#recession"
by <- 'hour'
tweets <- search_tweets(q = search_term ,
n = 100000, retryonratelimit = TRUE, lang="en", include_rts = FALSE)
tweets <- tweets %>% as.data.frame()
nrow(tweets)
tweets <- searchTwitter(searchString = search_term, n=100000, lang="en", retryonratelimit = TRUE)
#Note: Replace below with your credentials following above reference
api_key <- "oJ6CmPerh7jHxP5ZHazRuez1U"
api_secret_key <- "zVKRrtlXKdWBPsFKaBs4h6z9cS0nQrZwoRI6dmhfDcdV3qDS20"
access_token <- "3914872158-vYvnOj6VE5ZpXcexVAu3YcMPh54uVdtfu4LLGVk"
access_token_secret <- "jhC9VDAZNbFj4mpPoWbRU3eq1KCrghMqIeiMpkCylcG8i"
### The following code setup direct authentication to access twitter
setup_twitter_oauth(api_key, api_secret_key, access_token, access_token_secret)
search_term <- "#recession"
by <- 'hour'
tweets <- searchTwitter(searchString = search_term, n=100000, lang="en", retryonratelimit = TRUE)
tweets <- searchTwitter(searchString = search_term, n=100000, lang="en")
tweets <- tweets %>% as.data.frame()
tweets
tweets.df <- twListToDF(tweets)
tweets.df
tweets[1,]
tweets.df[1,]
tweetsDF <- twListToDF(tweets)
tweetsDF <- tweetsDF %>% filter(isRetweet == "FALSE")
tweetsDF
names(tweetsDF)
names(tweetsDF)
tweetsSubSet <- tweetsDF %>% select(created, text, longitude, latitude)
nrow(tweetsSubSet)
all_states <- map_data("state")
data$region <- tolower(data$Area_Name)
all_states
tweetsSubSet
tweetsSubSet <- tweetsDF %>% select(created, text)
nrow(tweetsSubSet)
# Ignore graphical Parameters to avoid input errors
tweetsSubSet$text <- str_replace_all(tweetsSubSet$text,"[^[:graph:]]", " ")
tweetsSubSet$text <- cleanText(tweetsSubSet$text)
tweetsSubSet %>% head()
tweetsSubSet <- tweetsSubSet %>%
mutate(Created_At_Round = created %>% round(units = 'hours') %>% as.POSIXct())
tweetsSubSet %>% pull(created_at) %>% min()
tweetsSubSet %>% pull(created) %>% min()
tweetsSubSet %>% pull(created) %>% max()
plt <- tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_light() +
geom_line() +
xlab(label = 'Date') +
ylab(label = NULL) +
ggtitle(label = 'Number of Tweets per Hour')
plt %>% ggplotly()
by
rtweet::ts_plot(tweetsSubSet, by = "hour", trim = 1) + geom_point() +
theme_minimal() + labs(title = paste0('Tweets mentioning "',
search_term,'" by ',by),
x = 'Date', y = 'Count', caption = 'Source: Twitter API')
tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_light() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
ggtitle(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
search_term
tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_light() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_light() +
geom_point() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_fivethirtyeight() +
geom_point() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_wsj() +
geom_point() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
plt %>% ggplotly()
plt <- tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_wsj() +
geom_point() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
plt %>% ggplotly()
tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_economist() +
geom_point() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_economist_white() +
geom_point() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
tweetHourPlot <- tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_wsj() +
geom_point() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
tweetHourPlot %>% ggplotly()
tweetHourPlot
tweetHourPlot
tweetHourPlot
tweetHourPlot <- tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_economist() +
geom_point() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
tweetHourPlot
tweetHourPlot
tweetHourPlot
tweetHourPlot %>% plotly()
tweetHourPlot <- tweetsSubSet %>%
count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_economist() +
geom_point() +
geom_line() +
xlab(label = 'Date') +
ylab(label = 'Count') +
labs(title = paste0('Tweets mentioning "',
search_term,'" by hour') , caption = 'Source: Twitter API')
tweetHourPlot %>% ggplotly()
### Retrieving positive and negative words
positive <- scan('OpinionLexiconEnglish/positive-words.txt', what = 'character', comment.char = ';')
negative <- scan('OpinionLexiconEnglish/negative-words.txt', what = 'character', comment.char = ';')
# add our list of words below as you wish if missing in above read lists
positiveWords <- c(positive,'upgrade','Congrats','prizes','prize','thanks','thnx',
'Grt','gr8','plz','trending','recovering','brainstorm','leader')
negativeWords <- c(negative,'wtf','wait','waiting','epicfail','Fight','fighting',
'arrest','no','not')
sentimentScore <- function(sentences, pos.words, neg.words){
# require(plyr)
# require(stringr)
# we are giving vector of sentences as input.
# plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
# sentences = tweetsSubSet$text
# pos.words = positiveWords
# neg.words = negativeWords
scores <- lapply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub() function:
sentence <- gsub('https://','',sentence)
sentence <- gsub('http://','',sentence)
sentence <- gsub('[^[:graph:]]', ' ',sentence)
sentence <- gsub('[[:punct:]]', '', sentence)
sentence <- gsub('[[:cntrl:]]', '', sentence)
sentence <- gsub('\\d+', '', sentence)
sentence <- str_replace_all(sentence,"[^[:graph:]]", " ")
# and convert to lower case:
sentence <- tolower(sentence)
# split into words. str_split is in the stringr package
word.list <- str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words <- unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches <- match(words, pos.words)
neg.matches <- match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
# TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words)
scores <- unlist(scores)
sentences <- unlist(sentences)
scores.df <- data.frame(score=scores, text=sentences)
return(scores.df)
}
analysisSentiment <- sentimentScore(sentences = tweetsSubSet$text, pos.words = positiveWords,
neg.words = negativeWords)
analysisSentiment %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "lightblue")+
ylab("Frequency") +
xlab("sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
ggeasy::easy_center_title()
analysisSentiment %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "lightblue")+
ylab("Frequency") +
xlab("Sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
theme_classic() +
ggeasy::easy_center_title()
analysisSentiment %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "royalblue")+
ylab("Frequency") +
xlab("Sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
theme_classic() +
ggeasy::easy_center_title()
analysisSentiment %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "olivedrab")+
ylab("Frequency") +
xlab("Sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
theme_classic() +
ggeasy::easy_center_title()
analysisSentiment %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "olivedrab")+
ylab("Frequency") +
xlab("Sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
theme_classic() +
ggeasy::easy_center_title()
neutral <- length(which(analysisSentiment$score == 0))
positive <- length(which(analysisSentiment$score > 0))
negative <- length(which(analysisSentiment$score < 0))
Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
geom_bar(stat = "identity", aes(fill = Sentiment))+
ggtitle("Barplot of Sentiment for recession")
neutral <- length(which(analysisSentiment$score == 0))
positive <- length(which(analysisSentiment$score > 0))
negative <- length(which(analysisSentiment$score < 0))
Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
geom_bar(stat = "identity", aes(fill = Sentiment))+
ggtitle("Barplot of Sentiment for recession")+
theme_classic() +
ggeasy::easy_center_title()
text_corpus <- Corpus(VectorSource(tweetsSubSet$text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, function(x)removeWords(x,stopwords("english")))
text_corpus <- tm_map(text_corpus, removeWords, c("recession", "will", "can"))
stopwords("english")
textCorpus <- Corpus(VectorSource(tweetsSubSet$text))
textCorpus <- tm_map(textCorpus, content_transformer(tolower))
textCorpus <- tm_map(textCorpus, function(x)removeWords(x,stopwords("english")))
textCorpus <- tm_map(textCorpus, removeWords, c("recession", "will", "can"))
ternDocMat <- TermDocumentMatrix(textCorpus)
ternDocMat <- as.matrix(ternDocMat)
textCorpus <- Corpus(VectorSource(tweetsSubSet$text))
textCorpus <- tm_map(textCorpus, content_transformer(tolower))
textCorpus <- tm_map(textCorpus, function(x)removeWords(x,stopwords("english")))
textCorpus <- tm_map(textCorpus, removeWords, c("recession", "will", "can"))
termDocMat <- TermDocumentMatrix(textCorpus)
termDocMat <- as.matrix(termDocMat)
termDocMat <- sort(rowSums(termDocMat), decreasing = TRUE)
termDocMat <- data.frame(word = names(termDocMat), freq = tdm)
set.seed(123)
wordcloud(textCorpus, min.freq = 5, max.words = 2000,
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
wordcloud(textCorpus, min.freq = 5, max.words = 500,
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
?wordcloud
wordcloud(textCorpus, min.freq = 5, max.words = 200,
colors=brewer.pal(8, "Dark2"), random.color = F, random.order = F)
wordcloud(textCorpus, min.freq = 5, max.words = 200,
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
ggplot(tdm[1:20,], aes(x=reorder(word, freq), y=freq)) +
geom_bar(stat="identity") +
xlab("Terms") +
ylab("Count") +
coord_flip() +
theme(axis.text=element_text(size=7)) +
ggtitle('Most common word frequency plot') +
ggeasy::easy_center_title()
termDocMat
termDocMat %>% ggplot(aes(x=reorder(word, freq), y=freq, fill = word)) +
geom_bar(stat="identity") +
xlab("Terms") +
ylab("Count") +
coord_flip() +
theme(axis.text=element_text(size=7)) +
ggtitle('Most common word frequency plot') +
ggeasy::easy_center_title()
ggplot(tdm[1:20,], aes(x=reorder(word, freq), y=freq, fill = word)) +
geom_bar(stat="identity") +
xlab("Terms") +
ylab("Count") +
coord_flip() +
theme(axis.text=element_text(size=7)) +
ggtitle('Most common word frequency plot') +
ggeasy::easy_center_title()
ggplot(tdm[1:20,], aes(x=reorder(word, freq), y=freq, fill = word)) +
geom_bar(stat="identity") +
xlab("Terms") +
ylab("Count") +
coord_flip() +
theme(axis.text=element_text(size=7)) +
ggtitle('Most common word frequency plot') +
ggeasy::easy_center_title() +
theme(legend.position="none")
analysisSentiment
analysisSentiment %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "olivedrab")+
ylab("Frequency") +
xlab("Sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
theme_classic() +
ggeasy::easy_center_title()
neutral
positive
negative
Sentiment
